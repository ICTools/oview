# Guide d'Initialisation Interactive

## Vue d'ensemble

`oview init` est maintenant **interactif** ! Plus besoin d'√©diter manuellement `.oview/project.yaml`.

## Workflow

```bash
cd ~/Documents/mon-projet
oview init
```

### √âtape 1 : D√©tection automatique

```
üîç Initializing oview for this project...

üìÅ Creating .oview directory structure...
   ‚úì Directory structure created
üîé Detecting project stack...
   ‚úì Stack detected:
     - Symfony
     - Docker
     - Makefile
     - Frontend: [Symfony UX]
     - Languages: [PHP JavaScript]
```

### √âtape 2 : Configuration des embeddings

```
ü§ñ Configuration interactive

üìä Configuration des embeddings (vecteurs s√©mantiques)

Les embeddings permettent la recherche s√©mantique dans votre code.

Providers disponibles:
  1. stub         - Placeholder (hash, pas de s√©mantique) - Gratuit
  2. openai       - OpenAI API (haute qualit√©) - ~$0.02/1M tokens
  3. ollama       - Local (priv√©, gratuit) - N√©cessite installation

Choisir provider [1-3] (d√©faut: 1):
```

**Choix recommand√©s :**

#### Pour d√©veloppement local (priv√©, gratuit)
```
Choisir provider [1-3]: 3

Mod√®les Ollama populaires:
  1. nomic-embed-text   - 768 dim, 274 MB (recommand√©)
  2. mxbai-embed-large  - 1024 dim, 669 MB
  3. bge-code           - 768 dim, optimis√© code
  4. all-minilm         - 384 dim, 45 MB (rapide)

Choisir mod√®le [1-4]: 1
Base URL Ollama (d√©faut: http://localhost:11434): [Enter]

üí° Avant d'indexer, lancez: ollama serve && ollama pull nomic-embed-text
```

#### Pour production (qualit√© maximale)
```
Choisir provider [1-3]: 2

Mod√®les OpenAI disponibles:
  1. text-embedding-3-small  - $0.02/1M tokens, 1536 dim (recommand√©)
  2. text-embedding-3-large  - $0.13/1M tokens, 3072 dim (meilleure qualit√©)
  3. text-embedding-ada-002  - $0.10/1M tokens, 1536 dim (ancien)

Choisir mod√®le [1-3]: 1

üí° N'oubliez pas de configurer OPENAI_API_KEY dans votre environnement
```

#### Pour tests (sans setup)
```
Choisir provider [1-3]: 1

‚ÑπÔ∏è  Stub: Pas de s√©mantique, uniquement pour tester l'infrastructure
```

### √âtape 3 : Configuration du LLM

```
ü§ñ Configuration du LLM (agent AI)

Le LLM sera utilis√© par les agents pour analyser et modifier le code.

Providers disponibles:
  1. claude-code   - Claude Code CLI (Sonnet 4.5) - Int√©gr√©
  2. claude-api    - Claude API (Anthropic) - N√©cessite cl√© API
  3. openai        - OpenAI API (GPT-4, etc.) - N√©cessite cl√© API
  4. ollama        - Local (Llama 3, etc.) - Gratuit

Choisir provider [1-4] (d√©faut: 1):
```

**Choix recommand√©s :**

#### Claude Code (d√©faut, recommand√©)
```
Choisir provider [1-4]: 1

‚úÖ Claude Code: Utilise le CLI actuel (recommand√©)
```

#### Claude API (si vous pr√©f√©rez l'API)
```
Choisir provider [1-4]: 2

Mod√®les Claude API:
  1. claude-sonnet-4.5    - Dernier, √©quilibr√© (recommand√©)
  2. claude-opus-4.5      - Maximum qualit√©
  3. claude-haiku-4       - Rapide et √©conomique

Choisir mod√®le [1-3]: 1

üí° Configurez ANTHROPIC_API_KEY dans votre environnement
```

#### Ollama (local, gratuit)
```
Choisir provider [1-4]: 4

Mod√®les Ollama populaires:
  1. llama3.1:70b      - Haute qualit√©
  2. llama3.1:8b       - Rapide
  3. codellama:34b     - Optimis√© code
  4. deepseek-coder    - Sp√©cialis√© code

Choisir mod√®le [1-4]: 2
Base URL Ollama (d√©faut: http://localhost:11434): [Enter]

üí° Avant d'utiliser, lancez: ollama serve && ollama pull llama3.1:8b
```

### √âtape 4 : Finalisation

```
üìù Creating project configuration...
   ‚úì Project config saved (slug: mon-projet)
üìã Creating RAG configuration...
   ‚úì RAG config saved
üìä Creating index manifests...
   ‚úì Index manifests created
ü§ñ Generating Claude agent instruction files...
   ‚úì Agent files generated

‚úÖ Initialization complete!
```

## R√©sultat dans `.oview/project.yaml`

```yaml
project_id: abc123
project_slug: mon-projet
embeddings:
  provider: ollama
  model: nomic-embed-text
  dim: 768
  base_url: http://localhost:11434
llm:
  provider: claude-code
  model: claude-sonnet-4.5
```

## Mode non-interactif

Pour les scripts et CI/CD :

```bash
oview init --non-interactive
```

Utilise les valeurs par d√©faut :
- Embeddings : stub
- LLM : claude-code (Sonnet 4.5)

## Reconfiguration

Si vous voulez changer la config apr√®s coup :

### Option 1 : R√©initialiser (recommand√©)
```bash
oview init --force
# R√©pond aux questions interactives
```

### Option 2 : √âdition manuelle
```bash
vim .oview/project.yaml
# Modifiez les sections embeddings et llm
```

Puis :
```bash
# Si vous avez chang√© le mod√®le d'embeddings
oview index  # R√©indexe avec le nouveau mod√®le

# Si vous avez chang√© le LLM
# Rien √† faire, il sera utilis√© au prochain appel d'agent
```

## Exemples de combinaisons

### Dev local full open-source
```
Embeddings: ollama / nomic-embed-text
LLM:        ollama / llama3.1:8b
```

**Setup :**
```bash
ollama serve &
ollama pull nomic-embed-text
ollama pull llama3.1:8b
```

**Avantages :**
- ‚úÖ 100% gratuit
- ‚úÖ 100% priv√©
- ‚úÖ Pas besoin d'Internet

**Inconv√©nients :**
- ‚ö†Ô∏è N√©cessite RAM/GPU
- ‚ö†Ô∏è Plus lent

### Production qualit√© maximale
```
Embeddings: openai / text-embedding-3-small
LLM:        claude-code / claude-sonnet-4.5
```

**Setup :**
```bash
export OPENAI_API_KEY="sk-..."
# Claude Code d√©j√† configur√©
```

**Avantages :**
- ‚úÖ Qualit√© maximale
- ‚úÖ Rapide
- ‚úÖ Claude Code int√©gr√©

**Inconv√©nients :**
- üí∞ ~$0.02 par 1M tokens embeddings
- üí∞ Co√ªt Claude selon usage

### Compromis (recommand√©)
```
Embeddings: ollama / nomic-embed-text  (local, gratuit)
LLM:        claude-code / claude-sonnet-4.5  (int√©gr√©)
```

**Setup :**
```bash
ollama serve &
ollama pull nomic-embed-text
# Claude Code d√©j√† configur√©
```

**Avantages :**
- ‚úÖ Embeddings gratuits et priv√©s
- ‚úÖ LLM de haute qualit√©
- ‚úÖ Bon √©quilibre

### Tests et d√©veloppement infra
```
Embeddings: stub / stub-hash-based
LLM:        claude-code / claude-sonnet-4.5
```

**Aucun setup n√©cessaire !**

Parfait pour :
- Tester l'infrastructure
- D√©velopper des features
- CI/CD

## Navigation interactive

**Entr√©e vide = valeur par d√©faut**
```
Choisir provider [1-3] (d√©faut: 1): [Enter]
‚Üí Utilise option 1
```

**Num√©ro OU nom**
```
Choisir mod√®le [1-4]: 3
‚Üí Utilise option 3

Choisir mod√®le [1-4]: nomic-embed-text
‚Üí Trouve et utilise nomic-embed-text
```

**Case insensitive**
```
Choisir provider: OPENAI
‚Üí Fonctionne
```

## V√©rification de la config

```bash
# Voir la config compl√®te
cat .oview/project.yaml

# Voir uniquement embeddings
cat .oview/project.yaml | grep -A 5 "embeddings:"

# Voir uniquement LLM
cat .oview/project.yaml | grep -A 4 "llm:"
```

## Int√©gration avec le reste du workflow

### Apr√®s init avec Ollama embeddings

```bash
# 1. Init (fait)
oview init
# ‚Üí Choisit ollama / nomic-embed-text

# 2. Setup Ollama
ollama serve &
ollama pull nomic-embed-text

# 3. Adapter la DB pour 768 dimensions
oview up
docker exec oview-postgres psql -U oview -d oview_mon-projet -c \
  "ALTER TABLE chunks ALTER COLUMN embedding TYPE vector(768);"

# 4. Indexer
oview index
```

### Apr√®s init avec OpenAI embeddings

```bash
# 1. Init (fait)
oview init
# ‚Üí Choisit openai / text-embedding-3-small

# 2. Configurer la cl√©
export OPENAI_API_KEY="sk-..."

# 3. Setup DB
oview up

# 4. Indexer
oview index
```

### Apr√®s init avec stub

```bash
# 1. Init (fait)
oview init
# ‚Üí Choisit stub

# 2. Setup DB
oview up

# 3. Indexer (stub, rapide)
oview index

# 4. Plus tard, passer √† de vrais embeddings
vim .oview/project.yaml
# Changer provider + model + dim

# 5. R√©indexer
oview index
```

## FAQ

### Puis-je sauter l'interactif ?

Oui :
```bash
oview init --non-interactive
```

### Puis-je relancer init apr√®s ?

Oui :
```bash
oview init --force
```

R√©pond aux questions, √©crase la config.

### Les choix sont-ils valid√©s ?

Oui, seuls les choix valides sont accept√©s. En cas d'erreur, la valeur par d√©faut est utilis√©e.

### Puis-je √©diter manuellement apr√®s ?

Oui, `.oview/project.yaml` reste √©ditable.

### Que se passe-t-il si j'appuie juste sur Entr√©e ?

La valeur par d√©faut est utilis√©e (indiqu√©e entre parenth√®ses).

### Les API keys sont-elles stock√©es ?

**Non** (par d√©faut). Le champ `api_key` existe mais est vide.

**Recommandation :** Utilisez les variables d'environnement :
- `OPENAI_API_KEY`
- `ANTHROPIC_API_KEY`

Si vous voulez vraiment stocker dans le fichier :
```yaml
embeddings:
  api_key: sk-...  # ‚ö†Ô∏è Ne commitez JAMAIS ce fichier avec une cl√© !
```

### Puis-je utiliser des mod√®les custom ?

Oui ! Si vous tapez un nom qui n'est pas dans la liste, il sera utilis√© tel quel.

Exemple :
```
Choisir mod√®le: mon-modele-custom
‚Üí Utilise "mon-modele-custom"
```

---

**L'init interactif rend oview accessible aux d√©butants tout en restant flexible pour les experts !** üöÄ
