# Guide des Embeddings pour oview

## Vue d'ensemble

Les **embeddings** sont des vecteurs qui capturent le **sens s√©mantique** du code. Ils permettent de faire des recherches intelligentes comme "comment l'authentification fonctionne ?" au lieu de simplement chercher le mot "auth".

## Options disponibles

### 1. Stub (par d√©faut - MVP) ‚ö†Ô∏è

**Utilisation :**
```bash
oview index
# ou explicitement
oview index --embeddings=stub
```

**Caract√©ristiques :**
- ‚ùå **Aucune compr√©hension s√©mantique**
- ‚úÖ Gratuit et rapide
- ‚úÖ Pas besoin d'API ou de configuration
- ‚úÖ D√©terministe (m√™me texte = m√™me vecteur)

**Quand l'utiliser :** Tests et d√©veloppement de l'infrastructure uniquement.

---

### 2. OpenAI (Recommand√©) ‚úÖ

**Mod√®les disponibles :**
- `text-embedding-3-small` (d√©faut) - **$0.02 / 1M tokens** ‚≠ê Recommand√©
- `text-embedding-3-large` - $0.13 / 1M tokens (meilleur qualit√©)
- `text-embedding-ada-002` - $0.10 / 1M tokens (ancien)

**Installation :**
1. Cr√©ez un compte sur https://platform.openai.com
2. Obtenez votre cl√© API
3. Configurez la variable d'environnement :

```bash
export OPENAI_API_KEY="sk-..."
```

Ou ajoutez dans `~/.zshrc` :
```bash
echo 'export OPENAI_API_KEY="sk-..."' >> ~/.zshrc
source ~/.zshrc
```

**Utilisation :**

```bash
# Via variable d'environnement (recommand√©)
oview index --embeddings=openai

# Via flag
oview index --embeddings=openai --openai-key="sk-..."

# Avec un mod√®le sp√©cifique
OPENAI_MODEL="text-embedding-3-large" oview index --embeddings=openai
```

**Co√ªt estim√© pour votre projet (7840 chunks) :**
- Avec text-embedding-3-small : **~$0.12** (5.7 MB ‚âà 1.4M tokens)
- Indexation compl√®te : quelques minutes
- R√©indexation incr√©mentale : quasi gratuite

**Avantages :**
- ‚úÖ Qualit√© exceptionnelle
- ‚úÖ Tr√®s rapide (API)
- ‚úÖ Pas d'infrastructure √† g√©rer
- ‚úÖ Support multilingue (PHP, JS, commentaires en fran√ßais)

---

### 3. Ollama (Local - Gratuit) üè†

**Installation d'Ollama :**

```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# D√©marrer le service
ollama serve
```

**T√©l√©charger un mod√®le :**

```bash
# Mod√®le recommand√© (768 dimensions, 274 MB)
ollama pull nomic-embed-text

# Alternatives
ollama pull mxbai-embed-large  # 1024 dimensions, 669 MB
ollama pull all-minilm         # 384 dimensions, 45 MB (plus rapide)
```

**Utilisation :**

```bash
# Avec le mod√®le par d√©faut (nomic-embed-text)
oview index --embeddings=ollama

# Avec un mod√®le sp√©cifique
oview index --embeddings=ollama --ollama-model=mxbai-embed-large

# Avec une URL custom
oview index --embeddings=ollama --ollama-url=http://localhost:11434
```

**Note importante sur les dimensions :**
Si vous utilisez Ollama, vous devrez **adapter le sch√©ma de la base** :

```bash
# 1. V√©rifier la dimension du mod√®le
ollama show nomic-embed-text | grep -i dimension
# Output: 768 dimensions

# 2. Adapter la table chunks
docker exec oview-postgres psql -U oview -d oview_chapitreneuf -c \
  "ALTER TABLE chunks ALTER COLUMN embedding TYPE vector(768);"

# 3. R√©indexer
oview index --embeddings=ollama
```

**Avantages :**
- ‚úÖ Gratuit et priv√©
- ‚úÖ Pas besoin d'Internet
- ‚úÖ Donn√©es ne quittent pas votre machine
- ‚úÖ Pas de limite de tokens

**Inconv√©nients :**
- ‚ö†Ô∏è Plus lent que l'API OpenAI
- ‚ö†Ô∏è Qualit√© l√©g√®rement inf√©rieure
- ‚ö†Ô∏è N√©cessite de la RAM (2-4 GB)

---

## Comparaison rapide

| Crit√®re | Stub | OpenAI | Ollama |
|---------|------|--------|--------|
| **Qualit√©** | ‚ùå Aucune | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Co√ªt** | Gratuit | ~$0.10 | Gratuit |
| **Vitesse** | ‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö° | ‚ö°‚ö° |
| **Setup** | Aucun | Cl√© API | Installation |
| **Priv√©** | ‚úÖ | ‚ùå | ‚úÖ |
| **Internet** | Non | Oui | Non |
| **Recommand√© pour** | Tests infra | Production | Dev local |

## Workflow recommand√©

### Pour le d√©veloppement (votre cas actuel)

**Option 1 : OpenAI (rapide √† tester)**
```bash
# 1. Configurez la cl√©
export OPENAI_API_KEY="sk-..."

# 2. R√©indexez avec de vrais embeddings
cd ~/Documents/chapitreneuf
oview index --embeddings=openai

# Co√ªt : ~$0.12 pour vos 797 fichiers
```

**Option 2 : Ollama (gratuit mais plus d'installation)**
```bash
# 1. Installez Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. D√©marrez le service
ollama serve &

# 3. T√©l√©chargez le mod√®le
ollama pull nomic-embed-text

# 4. Adaptez la base (768 dimensions au lieu de 1536)
docker exec oview-postgres psql -U oview -d oview_chapitreneuf -c \
  "ALTER TABLE chunks ALTER COLUMN embedding TYPE vector(768);"

# 5. R√©indexez
cd ~/Documents/chapitreneuf
oview index --embeddings=ollama
```

### Pour la production

Utilisez **OpenAI** pour la qualit√© et la simplicit√©, avec un m√©canisme de cache pour minimiser les co√ªts :
- Indexation compl√®te : une fois
- R√©indexations : uniquement les fichiers modifi√©s (TODO: √† impl√©menter)

---

## V√©rification

Apr√®s r√©indexation avec de vrais embeddings, v√©rifiez :

```bash
# Comptez les chunks
docker exec oview-postgres psql -U oview -d oview_chapitreneuf -c \
  "SELECT COUNT(*) FROM chunks;"

# V√©rifiez qu'il y a bien des embeddings non-nuls
docker exec oview-postgres psql -U oview -d oview_chapitreneuf -c \
  "SELECT path, LENGTH(embedding::text) as embedding_size FROM chunks LIMIT 5;"
```

Les embeddings OpenAI/Ollama auront une taille ~20-30KB (texte du vecteur), alors que les stubs sont plus courts.

---

## D√©pannage

### "OpenAI API error: 401"
‚Üí Cl√© API invalide. V√©rifiez votre `OPENAI_API_KEY`

### "Ollama API request failed: connection refused"
‚Üí Ollama n'est pas d√©marr√©. Lancez `ollama serve`

### "Model not found"
‚Üí Le mod√®le n'est pas t√©l√©charg√©. Lancez `ollama pull nomic-embed-text`

### Embeddings trop longs (d√©passement de token limit)
‚Üí Les chunks sont automatiquement tronqu√©s √† 30k caract√®res

---

## Performance

### Temps d'indexation estim√© pour 797 fichiers (7840 chunks)

- **Stub** : ~1 minute (votre temps actuel)
- **OpenAI** : ~5-10 minutes (limit√© par l'API rate limit)
- **Ollama** : ~15-30 minutes (d√©pend de votre CPU/GPU)

### Optimisations futures possibles
- Indexation parall√®le (batch requests)
- Cache des embeddings par hash de contenu
- Indexation incr√©mentale (uniquement fichiers modifi√©s)

---

## Prochaines √©tapes

Une fois les vrais embeddings en place, vous pourrez :
1. Impl√©menter un syst√®me de requ√™tes RAG
2. Faire des recherches s√©mantiques : "comment fonctionne l'authentification ?"
3. Utiliser les agents Claude avec contexte RAG pertinent
4. Construire des workflows n8n qui utilisent le contexte du code

**Voulez-vous que je vous aide √† impl√©menter un de ces points ?**
